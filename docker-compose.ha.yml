# This docker-compose file is provided as an example to create a Docker Swarm based MSActivator setup
version: "3.8"

x-es-configuration: &es-configuration
    ES_CREDENTIALS: c3VwZXJ1c2VyOnheWnl1R002fnU9K2ZZMkc=
    ES_SERVERS: "msa-es"

x-logging: &logging
  driver: "json-file"
  options:
    mode: non-blocking
    max-buffer-size: "4m"
    max-size: "10m"
    max-file: "5"

services:
  msa-front:
    image: openmsa/openmsa:msa2-front-2.8.7-682b92484210f8e48f8034af7ea8fcb2c435116f
    depends_on:
      - msa-api
      - msa-ui
    healthcheck:
      test: ["CMD-SHELL", "curl -k --fail https://localhost"]
      timeout: 2s
      retries: 10
      interval: 10s
      start_period: 30s
    deploy:
      replicas: 1
      placement:
        max_replicas_per_node: 1
        constraints:
          - "node.labels.worker==app"
    ports:
      - target: 80
        published: 80
        protocol: tcp
        mode: ingress
      - target: 443
        published: 443
        protocol: tcp
        mode: ingress
      - target: 162
        published: 162
        protocol: udp
        mode: ingress
      - target: 69
        published: 69
        protocol: udp
        mode: ingress
      - "5200-5200:5200-5200/udp"
    logging:
      driver: "json-file"
      options:
        mode: non-blocking
        max-buffer-size: "4m"
        max-size: "10m"
        max-file: "5"
    volumes:
      - "/mnt/NASVolume/msa_front_conf:/etc/nginx/custom_conf.d"
      - "/mnt/NASVolume/msa_front:/etc/nginx/ssl"
    #
    # uncomment one of the 2 sections below when installing a custom certificate
    # - Docker standard standalone installation
    #volumes:
    #    - "msa_front:/etc/nginx/ssl"
    # - Docker Swarm HA installation
    #volumes:
    #    - "/mnt/NASVolume/msa_front:/etc/nginx/ssl"

  db:
    image: openmsa/openmsa:msa2-db-2.8.7-c5a834e55c9508434758d7947635640fa978aa1d
    healthcheck:
      test: ["CMD-SHELL", "/usr/pgsql-12/bin/pg_isready -h localhost"]
      timeout: 20s
      interval: 30s
      retries: 5
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure
      placement:
        max_replicas_per_node: 1
        constraints:
          - "node.labels.worker==db"
    environment:
      CAMUNDA_PASSWORD: camunda
      CAMUNDA_DB: process-engine
      CAMUNDA_USER: camunda
      KEY_VAULT_USER: key_vault
      KEY_VAULT_DB: key_vault
      PG_MODE: primary
      PG_PRIMARY_USER: postgres
      PG_PRIMARY_PASSWORD: my_db_password
      PG_USER: postgres
      PG_PASSWORD: my_db_password
      PG_DATABASE: POSTGRESQL
      PG_ROOT_PASSWORD: my_db_password
      PG_PRIMARY_PORT: 5432
      MAX_CONNECTIONS: 1600
    volumes:
      - "/mnt/NASVolume/msa_db:/pgsqldata/pgsql"
    logging:
      <<: *logging

  db-replica:
    image: openmsa/openmsa:msa2-db-2.8.7-c5a834e55c9508434758d7947635640fa978aa1d
    healthcheck:
      test: ["CMD-SHELL", "/usr/pgsql-12/bin/pg_isready -h localhost"]
      timeout: 20s
    deploy:
      replicas: 1
      restart_policy:
        condition: any
      placement:
        max_replicas_per_node: 1
        constraints:
          - "node.labels.worker==db-replica"
    environment:
      CAMUNDA_PASSWORD: camunda
      CAMUNDA_DB: process-engine
      CAMUNDA_USER: camunda
      KEY_VAULT_USER: key_vault
      KEY_VAULT_DB: key_vault
      PG_MODE: replica
      PG_PRIMARY_USER: postgres
      PG_PRIMARY_PASSWORD: my_db_password
      PG_USER: postgres
      PG_PASSWORD: my_db_password
      PG_DATABASE: POSTGRESQL
      PG_ROOT_PASSWORD: my_db_password
      PG_PRIMARY_PORT: 5432
      PG_PRIMARY_HOST: db
    volumes:
      - "/etc/timezone:/etc/timezone:ro"
      - "/etc/localtime:/etc/localtime:ro"
    logging:
      <<: *logging

  msa-api:
    image: openmsa/openmsa:msa2-api-2.8.7-6e8c462fbea06596415bb024644598893e7220a3
    depends_on:
      - db
    healthcheck:
      test: ["CMD-SHELL", "curl --fail http://localhost:8480"]
    deploy:
      replicas: 3
      placement:
        max_replicas_per_node: 1
        constraints:
          - "node.labels.worker==app"
      update_config:
        parallelism: 1
      restart_policy:
        condition: on-failure
        max_attempts: 10
    environment:
      no_proxy: "127.0.0.1,localhost,::1,.repo,10.0.0.0/8,.tm.com.my"
      http_proxy: "http://10.242.225.20:8000/"
      https_proxy: "http://10.242.225.20:8000/"
      <<: *es-configuration
    volumes:
      - "/mnt/NASVolume/msa_dev:/opt/devops/"
      - "/mnt/NASVolume/rrd_repository:/opt/rrd"
      - "/mnt/NASVolume/msa_entities:/opt/fmc_entities"
      - "/mnt/NASVolume/msa_repository:/opt/fmc_repository"
      - "msa_api_logs:/opt/wildfly/logs/processLog"
      - "/mnt/NASVolume/msa_api_keystore:/etc/pki/jentreprise"
    logging:
      <<: *logging
    networks:
      default:
        aliases:
          - "msa_api"

  msa-ui:
    image: openmsa/openmsa:msa2-ui-2.8.7-1c9ccd157fef826dbeb34b65647628e0c9340310
    depends_on:
      - msa-api
    healthcheck:
      test: ["CMD-SHELL", "curl --fail http://localhost:8080"]
    deploy:
      replicas: 3
      placement:
        max_replicas_per_node: 1
        constraints:
          - "node.labels.worker==app"
    environment:
    - FEATURE_ADMIN=true
    - FEATURE_REPOSITORY=true
    - FEATURE_CONNECTION_STATUS=true
    - FEATURE_ALARMS=true
    - FEATURE_LICENCE=true
    - FEATURE_TOPOLOGY=true
    - FEATURE_MONITORING_PROFILES=true
    - FEATURE_PROFILE_AUDIT_LOGS=true
    - FEATURE_PERMISSION_PROFILES=true
    - FEATURE_AI_ML=false
    - FEATURE_MICROSERVICE_BULK_OPERATION=false
    - FEATURE_EDIT_VARIABLES_IN_MICROSERVICE_CONSOLE=true
    - FEATURE_WORKFLOW_OWNER=false
    - FEATURE_PERMISSION_PROFILE_LABELS=false
    - FEATURE_BPM=true
    - UBIQUBE_ES_SECURITY_DISABLED=true
    logging:
      <<: *logging
    networks:
      default:
        aliases:
          - "msa_ui"

  msa-rsyslog:
    depends_on:
      - msa-sms
    image: openmsa/openmsa:msa2-rsyslog-2.8.7-c7981110f0ff1ee153199786f4abf3c8f5e49db7
    # environment:
      # omfwd : enable action type for syslog Forwarding Output Module  (rely on source IP or the hostname to map with the managed entity)
      # https://www.rsyslog.com/doc/v8-stable/configuration/modules/omfwd.html
      # omudpspoof : UDP spoofing output module (rely on source IP to map with the managed entity)
      # https://www.rsyslog.com/doc/v8-stable/configuration/modules/omudpspoof.html
      # default
      # ACTIONTYPE: "omfwd"
      # configure a specific port for TLS. Default is 6514
      # TLS_SYSLOG_PORT: 6514
    deploy:
      replicas: 3
      placement:
        max_replicas_per_node: 1
        constraints:
          - "node.labels.worker==app"
    ports:
      - target: 514
        published: 514
        protocol: udp
        mode: host
      - target: 514
        published: 514
        protocol: tcp
        mode: host
      - target: 6514
        published: 6514
        protocol: tcp
        mode: host
    environment:
      ACTIONTYPE: "omfwd"
      TLS_SYSLOG_PORT: 6514
    volumes:
      - "rsyslog_conf:/etc"
    logging:
      <<: *logging
    networks:
      default:
        aliases:
          - "msa_rsyslog"

  msa-sms:
    image: openmsa/openmsa:msa2-sms-2.8.7-06ce61b27a31c4a5b4fcb555f15286d7a12bf072
    depends_on:
      - db
    healthcheck:
      timeout: 5s
      retries: 10
      interval: 10s
      start_period: 30s
      test: ["CMD-SHELL", "/etc/init.d/ubi-sms status | grep -q 'service seems UP' || exit 0"]
    environment:
      <<: *es-configuration
      CONTAINER_DOCKNAME: "{{.Task.Name}}.{{.Node.Hostname}}"
    volumes:
      - "/mnt/NASVolume/msa_dev:/opt/devops/"
      - "/mnt/NASVolume/msa_entities:/opt/fmc_entities"
      - "/mnt/NASVolume/msa_repository:/opt/fmc_repository"
      - "/mnt/NASVolume/rrd_repository:/opt/rrd"
      - "/mnt/NASVolume/msa_svn:/opt/svnroot"
      - "msa_bulkfiles:/opt/sms/spool/parser"
      - "msa_bulkfiles_err:/opt/sms/spool/parser-error"
      - "msa_sms_logs:/opt/sms/logs"
      - "/etc/timezone:/etc/timezone:ro"
      - "/etc/localtime:/etc/localtime:ro"
    deploy:
      replicas: 3
      placement:
        max_replicas_per_node: 1
        constraints:
          - "node.labels.worker==app"
    logging:
      <<: *logging
    networks:
      default:
        aliases:
          - "msa_sms"

  msa-bud:
    image: openmsa/openmsa:msa2-bud-2.8.7-04590d19dafe4acc2742c9a365e9dbdb80e45dfc
    depends_on:
      - db
    healthcheck:
      timeout: 5s
      retries: 10
      interval: 10s
      start_period: 30s
      test: ["CMD-SHELL", "/etc/init.d/ubi-bud status | grep -q 'service seems UP' || exit 1"]
    environment:
      - CONTAINER_DOCKNAME={{.Task.Name}}.{{.Node.Hostname}}
    deploy:
      replicas: 1
      placement:
        max_replicas_per_node: 1
        constraints:
          - "node.labels.worker==app"
    logging:
      <<: *logging
    networks:
      default:
        aliases:
          - "msa_bud"

  msa-alarm:
    depends_on:
      - db
      - msa-api
    image: openmsa/openmsa:msa2-alarm-2.8.7-a9b92668c96ee4a90e5447eb5af1c20b21971787
    healthcheck:
      timeout: 5s
      retries: 10
      interval: 10s
      start_period: 30s
      test: ["CMD-SHELL", "/etc/init.d/ubi-alarm status | grep -q 'service seems UP' || exit 1"]
    environment:
      <<: *es-configuration
      CONTAINER_DOCKNAME: "{{.Task.Name}}.{{.Node.Hostname}}"
    deploy:
      replicas: 3
      placement:
        max_replicas_per_node: 1
        constraints:
          - "node.labels.worker==app"
    volumes:
      - "msa_sms_logs:/opt/sms/logs"
      - "msa_alarmbulkfiles:/opt/sms/spool/alarms"
      - "msa_alarmbulkfiles_err:/opt/sms/spool/alarms-error"
      - "/etc/timezone:/etc/timezone:ro"
      - "/etc/localtime:/etc/localtime:ro"
    logging:
      <<: *logging
    networks:
      default:
        aliases:
          - "msa_alarm"

  msa-monitoring:
    image: openmsa/openmsa:msa2-monitoring-2.8.7-dfcc122af6838113bd9964b6e2a7587098ca8242
    healthcheck:
      timeout: 5s
      retries: 10
      interval: 10s
      start_period: 30s
      test: ["CMD-SHELL", "/etc/init.d/ubi-poll status | grep -q 'service seems UP' || exit 1"]
    depends_on:
      - db
      - msa-dev
      - msa-api
    environment:
      <<: *es-configuration
      CONTAINER_DOCKNAME: "{{.Task.Name}}.{{.Node.Hostname}}"
    volumes:
      - "/mnt/NASVolume/msa_dev:/opt/devops/"
      - "/mnt/NASVolume/msa_entities:/opt/fmc_entities"
      - "/mnt/NASVolume/msa_repository:/opt/fmc_repository"
      - "/mnt/NASVolume/rrd_repository:/opt/rrd"
      - "msa_bulkfiles:/opt/sms/spool/parser"
      - "msa_bulkfiles_err:/opt/sms/spool/parser-error"
      - "msa_sms_logs:/opt/sms/logs"
      - "/etc/timezone:/etc/timezone:ro"
      - "/etc/localtime:/etc/localtime:ro"
    deploy:
      replicas: 3
      placement:
        max_replicas_per_node: 1
        constraints:
          - "node.labels.worker==app"
    logging:
      <<: *logging
    networks:
      default:
        aliases:
          - "msa_monitoring"

  camunda:
    depends_on:
      - db
    image: openmsa/openmsa:msa2-camunda-2.8.7-23335a0df397d4bc3531366223cd5a239688ee0d
    deploy:
      replicas: 1
      placement:
        max_replicas_per_node: 1
        constraints:
          - "node.labels.worker==app"
    environment:
      DB_DRIVER: org.postgresql.Driver
      DB_URL: 'jdbc:postgresql://db:5432/process-engine'
      DB_USERNAME: camunda
      DB_PASSWORD: camunda
      DB_VALIDATE_ON_BORROW: 'true'
      WAIT_FOR: 'db:5432'
      WAIT_FOR_TIMEOUT: 60
    logging:
      <<: *logging

  msa-es:
    image: openmsa/openmsa:msa2-es-2.8.7-0bdbe159f733c75a5ea0696420ded038c87b5d6c
    deploy:
      replicas: 1
      placement:
        max_replicas_per_node: 1
        constraints:
          - "node.labels.worker==app"
    environment:
      node.roles: "ingest"
      node.name: "alpha_lab_client1"
      discovery.seed_hosts: "172.17.28.82,172.17.28.83"
      cluster.initial_master_nodes: "alpha_lab_node1"
      cluster.name: "alpha_lab"
      network.host: "0.0.0.0"
      http.host: "0.0.0.0"
      network.publish_host: "172.17.28.84"
      script.painless.regex.enabled: "true"
      bootstrap.memory_lock: "true"
      xpack.security.enabled: "true"
      xpack.security.transport.ssl.enabled: "true"
      xpack.security.transport.ssl.verification_mode: "certificate"
      xpack.security.transport.ssl.client_authentication: "required"
      xpack.security.transport.ssl.keystore.path: "elastic-certificates.p12"
      xpack.security.transport.ssl.truststore.path: "elastic-certificates.p12"
      ES_JAVA_OPTS: "-Xms2048m -Xmx2048m"
      <<: *es-configuration
    ulimits:
      memlock:
        soft: -1
        hard: -1
    networks:
      default:
        aliases:
          - "msa_es"
    ports:
      - "9300:9300"
      - "9200:9200"
      - "9300:9300/udp"
      - "9200:9200/udp"
    volumes:
      - "/etc/timezone:/etc/timezone:ro"
      - "/etc/localtime:/etc/localtime:ro"
      - "msa_es:/usr/share/elasticsearch/data"
      - "msa_es_config:/usr/share/elasticsearch/config"
    logging:
      <<: *logging

  msa-kibana:
    image: openmsa/openmsa:msa2-kibana-2.8.7-b5a0a64970f780ffddaf884f6bbff149431f0026
    ports:
      - "5601:5601"
    environment:
      NODE_OPTIONS: "--max-old-space-size=4096"
      ELASTICSEARCH_URL: "http://msa_es:9200"
      ELASTICSEARCH_HOSTS: "http://msa_es:9200"
      <<: *es-configuration
    deploy:
      replicas: 1
      placement:
        max_replicas_per_node: 1
        constraints:
          - "node.labels.worker==app"
    logging:
      <<: *logging
    networks:
      default:
        aliases:
          - "msa_kibana"

  msa-cerebro:
    image: openmsa/openmsa:msa2-cerebro-2.8.7-914750e000db1343d9972bfa6652da1efe4aa32f
    environment:
      AUTH_TYPE: basic
      BASIC_AUTH_USER: cerebro
      BASIC_AUTH_PWD: "N@X{M4tfw'5%)+35"
    entrypoint:
      - /opt/cerebro/bin/cerebro
      - -Dhosts.0.host=http://msa_es:9200
    ports:
      - "9000:9000"
    deploy:
      replicas: 1
      placement:
        max_replicas_per_node: 1
        constraints:
          - "node.labels.worker==app"
    logging:
      <<: *logging
    networks:
      default:
        aliases:
          - "msa_cerebro"

  msa-dev:
    image: openmsa/openmsa:msa2-linuxdev-2.8.7-6f2f80bb768a44ffad06dd7b4240c2a01c5c0e27
    deploy:
      replicas: 1
      placement:
        max_replicas_per_node: 1
        constraints:
          - "node.labels.worker==app"
    environment:
      - no_proxy=127.0.0.1,localhost,::1,.repo,10.0.0.0/8,.tm.com.my
      - http_proxy=http://user@172.17.28.150:8000/
      - https_proxy=http://user@172.17.28.150:8000/
    volumes:
      - "/mnt/NASVolume/msa_entities:/opt/fmc_entities"
      - "/mnt/NASVolume/msa_repository:/opt/fmc_repository"
      - "/mnt/NASVolume/msa_dev:/opt/devops/"
      - "/mnt/NASVolume/msa_svn:/opt/svnroot"
    logging:
      <<: *logging
    networks:
      default:
        aliases:
          - "msa_dev"

  telegraf:
    image: telegraf:latest
    deploy:
      replicas: 1
      placement:
        max_replicas_per_node: 1
        constraints:
          - "node.labels.worker==app"
    volumes:
      - "/mnt/NASVolume/telegraf:/etc/telegraf"
    ports:
      - '8125:8125'
    networks:
      default:
        aliases:
          - "telegraf"
    logging:
      <<: *logging

volumes:
  msa_bulkfiles:
  msa_bulkfiles_err:
  msa_alarmbulkfiles:
  msa_alarmbulkfiles_err:
  msa_sms_logs:
  msa_monitbulkfiles:
  msa_api_logs:
  msa_es:
  msa_es_config:
  rsyslog_conf:

networks:
  default:
