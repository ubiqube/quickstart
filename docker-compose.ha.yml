# This docker-compose file is provided as an example to create a Docker Swarm based MSActivator setup
version: "3.8"

x-es-configuration: &es-configuration
  ES_CREDENTIALS: c3VwZXJ1c2VyOnheWnl1R002fnU9K2ZZMkc=
  ES_SERVERS: "msa-es"

x-logging: &logging
  driver: "json-file"
  options:
    mode: non-blocking
    max-buffer-size: "4m"
    max-size: "10m"
    max-file: "5"

x-kafka-syslogs: &kafka-syslogs
  KAFKA_SERVER: "kafka:9094"
  KAFKA_TOPIC: "syslogs"

x-amqp-syslog: &amqp-syslog
  AMQP_SERVER: "msa-broker"
  AMQP_PORT: "5672"
  AMQP_ADDRESS: "core-engine.syslog"
  AMQP_USER: "artemis"
  AMQP_PASSWORD: "simetraehcapa"

x-placement_app: &placement_app
  replicas: 1
  placement:
    max_replicas_per_node: 1
    constraints:
      - "node.labels.worker==app"

# for containers that have to run with only one replica
x-placement_app_one_replica: &placement_app_one_replica
  replicas: 1
  placement:
    max_replicas_per_node: 1
    constraints:
      - "node.labels.worker==app"

# snmptrap and rsyslog ports are in mode host so we have to configure one replica per worker
x-placement_rsyslog: &placement_rsyslog
  replicas: 1
  placement:
    max_replicas_per_node: 1
    constraints:
      - "node.labels.worker==app"

x-placement_db: &placement_db
  replicas: 1
  placement:
    max_replicas_per_node: 1
    constraints:
      - "node.labels.worker==db"

x-healthcheck: &healthcheck
  timeout: 10s
  retries: 10
  interval: 30s
  start_period: 120s

services:
  msa-auth:
    image: openmsa/openmsa:msa2-auth-2.9.1-6326bc35b4f46878c02db3257fcd38cb58b42ddb
    environment:
      KEYCLOAK_ADMIN: admin
      KEYCLOAK_ADMIN_PASSWORD: ubiqube
    depends_on:
      - db
    healthcheck:
      test: timeout 10s bash -c ':> /dev/tcp/127.0.0.1/8080'
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 1m

  msa-front:
    image: openmsa/openmsa:msa2-front-2.9.1-67c4ceeb63d5b64641ece5da9cd59613cc8ae4f5
    depends_on:
      - msa-api
      - msa-ui
      - camunda
    healthcheck:
      <<: *healthcheck
      test: ["CMD-SHELL", "curl -k --fail https://localhost"]
    deploy:
      <<: *placement_app
    ports:
      - target: 80
        published: 80
        protocol: tcp
        mode: ingress
      - target: 443
        published: 443
        protocol: tcp
        mode: ingress
    logging:
      <<: *logging
    volumes:
      - "/mnt/NASVolume/msa_front_conf:/etc/nginx/custom_conf.d"
      #
      # uncomment one of the 2 sections below when installing a custom certificate
      # - Docker standard standalone installation
      # volumes:
      #    - "msa_front:/etc/nginx/ssl"
      # - Docker Swarm HA installation
      # volumes:
      #    - "/mnt/NASVolume/msa_front:/etc/nginx/ssl"

  db:
    image: openmsa/openmsa:msa2-db-2.9.1-79143368e7f42131221222ad855e43c6dd10f55e
    healthcheck:
      <<: *healthcheck
      test: ["CMD-SHELL", "/usr/pgsql-12/bin/pg_isready -h localhost"]
    deploy:
      <<: *placement_db
    environment:
      CAMUNDA_PASSWORD: camunda
      CAMUNDA_DB: process-engine
      CAMUNDA_USER: camunda
      KEY_VAULT_USER: key_vault
      KEY_VAULT_DB: key_vault
      PG_MODE: primary
      PG_PRIMARY_USER: postgres
      PG_PRIMARY_PASSWORD: my_db_password
      PG_USER: postgres
      PG_PASSWORD: my_db_password
      PG_DATABASE: POSTGRESQL
      PG_ROOT_PASSWORD: my_db_password
      PG_PRIMARY_PORT: 5432
      MAX_CONNECTIONS: 1600
    volumes:
      - "/mnt/NASVolume/msa_db:/pgsqldata/pgsql"
      - type: tmpfs
        target: /dev/shm
        tmpfs:
          size: 2000000000
    logging:
      <<: *logging

  db-replica:
    image: openmsa/openmsa:msa2-db-2.9.1-79143368e7f42131221222ad855e43c6dd10f55e
    healthcheck:
      <<: *healthcheck
      test: ["CMD-SHELL", "/usr/pgsql-12/bin/pg_isready -h localhost"]
    deploy:
      <<: *placement_db
    environment:
      CAMUNDA_PASSWORD: camunda
      CAMUNDA_DB: process-engine
      CAMUNDA_USER: camunda
      KEY_VAULT_USER: key_vault
      KEY_VAULT_DB: key_vault
      PG_MODE: replica
      PG_PRIMARY_USER: postgres
      PG_PRIMARY_PASSWORD: my_db_password
      PG_USER: postgres
      PG_PASSWORD: my_db_password
      PG_DATABASE: POSTGRESQL
      PG_ROOT_PASSWORD: my_db_password
      PG_PRIMARY_PORT: 5432
      PG_PRIMARY_HOST: db
    logging:
      <<: *logging

  msa-api:
    image: openmsa/openmsa:msa2-api-2.9.1-a05d5331fc37e4dd7de428c0ea3078603846bbc8
    depends_on:
      - db
    healthcheck:
      <<: *healthcheck
      test: ["CMD-SHELL", "curl --fail http://localhost:8480"]
    deploy:
      <<: *placement_app
    environment:
      <<: *es-configuration
      CONTAINER_DOCKNAME: "{{.Task.Name}}.{{.Node.Hostname}}"
      HOST_HOSTNAME: "{{.Node.Hostname}}"
    volumes:
      - "/mnt/NASVolume/msa_dev:/opt/devops/"
      - "/mnt/NASVolume/rrd_repository:/opt/rrd"
      - "/mnt/NASVolume/msa_entities:/opt/fmc_entities"
      - "/mnt/NASVolume/msa_repository:/opt/fmc_repository"
      - "/mnt/NASVolume/msa_api_keystore:/etc/pki/jentreprise"
      - "/mnt/NASVolume/msa_api_logs:/opt/wildfly/logs"
    logging:
      <<: *logging
    networks:
      default:
        aliases:
          - "msa_api"

  msa-ui:
    image: openmsa/openmsa:msa2-ui-2.9.1-cadd151898cbe65b91bf21a06008ab5596020213
    depends_on:
      - msa-api
    healthcheck:
      <<: *healthcheck
      test: ["CMD-SHELL", "curl --fail http://localhost:8080"]
    deploy:
      <<: *placement_app
    environment:
      - FEATURE_ADMIN=true
      - FEATURE_REPOSITORY=true
      - FEATURE_CONNECTION_STATUS=true
      - FEATURE_ALARMS=true
      - FEATURE_LICENCE=true
      - FEATURE_TOPOLOGY=true
      - FEATURE_MONITORING_PROFILES=true
      - FEATURE_PROFILE_AUDIT_LOGS=true
      - FEATURE_PERMISSION_PROFILES=true
      - FEATURE_AI_ML=false
      - FEATURE_MICROSERVICE_BULK_OPERATION=false
      - FEATURE_EDIT_VARIABLES_IN_MICROSERVICE_CONSOLE=true
      - FEATURE_WORKFLOW_OWNER=false
      - FEATURE_PERMISSION_PROFILE_LABELS=false
      - FEATURE_BPM=true
      - UBIQUBE_ES_SECURITY_DISABLED=true
      - FEATURE_ALARMS_AUTO_CLEARANCE=false
      - FEATURE_IMPORT_WITH_SAME_AND_UPPERRANK=true
      - FEATURE_REPOFOLDERLIST=[\"Datafiles\"]
    logging:
      <<: *logging
    networks:
      default:
        aliases:
          - "msa_ui"

  msa-rsyslog:
    image: openmsa/openmsa:msa2-rsyslog-2.9.1-fcb0e77b5e27821fb5f733baf2d8681ccfb4306b
    depends_on:
      - kafka
      - msa-broker
      - msa-parse
    healthcheck:
      <<: *healthcheck
      test: ["CMD-SHELL", "ps -p 1 -h -o%cpu | awk '{if ($$1 > 99) exit 1; else exit 0;}'"]
    deploy:
      <<: *placement_rsyslog
    environment:
      ACTIONTYPE: "omamqp1"
      <<: *amqp-syslog
    ports:
      # on docker swarm rsyslog port can support only one protocol (TCP or UDP) per port and MUST be in host mode
      - target: 514
        published: 514
        protocol: udp
        mode: host
      - target: 6514
        published: 6514
        protocol: tcp
        mode: host
    logging:
      <<: *logging
    networks:
      default:
        aliases:
          - "msa_rsyslog"

  msa-sms:
    image: openmsa/openmsa:msa2-sms-2.9.1-8aec36cc431cb1b424e6f4539aff68a0b14507e8
    depends_on:
      - db
      - msa-dev
    healthcheck:
      <<: *healthcheck
      test: ["CMD-SHELL", "/opt/sms/bin/sms -e ISALIVE -t 1 | grep -q OK || exit 1"]
    deploy:
      <<: *placement_app
    environment:
      <<: *es-configuration
      CONTAINER_DOCKNAME: "{{.Task.Name}}.{{.Node.Hostname}}"
      HOST_HOSTNAME: "{{.Node.Hostname}}"
    volumes:
      - "/mnt/NASVolume/msa_sms_logs:/opt/sms/logs"
      - "/mnt/NASVolume/msa_dev:/opt/devops/"
      - "/mnt/NASVolume/msa_entities:/opt/fmc_entities"
      - "/mnt/NASVolume/msa_repository:/opt/fmc_repository"
      - "/mnt/NASVolume/msa_svn:/opt/svnroot"
      - "msa_bulkfiles:/opt/sms/spool/parser"
      - "msa_bulkfiles_err:/opt/sms/spool/parser-error"
    ports:
      - target: 69
        published: 69
        protocol: udp
        mode: host
      - target: 5200
        published: 5200
        protocol: udp
        mode: host
    logging:
      <<: *logging
    networks:
      default:
        aliases:
          - "msa_sms"

  msa-parse:
    image: openmsa/openmsa:msa2-parse-2.9.1-100412daae7431d0930aca976158bc17e097b87d
    depends_on:
      - db
      - kafka
      - msa-broker
      - msa-es
      - msa-dev
    healthcheck:
      <<: *healthcheck
      test: ["CMD-SHELL", "/opt/sms/bin/sms -e ISALIVE -t 1 | grep -q OK || exit 1"]
    deploy:
      <<: *placement_app
    environment:
      <<: [*es-configuration, *amqp-syslog]
    logging:
      <<: *logging
    volumes:
      - "msa_sms_logs:/opt/sms/logs"
      - "/mnt/NASVolume/msa_dev:/opt/devops/"
      - "msa_parsebulkfiles:/opt/sms/spool/parser"
      - "msa_parsebulkfiles_err:/opt/sms/spool/parser-error"
    networks:
      default:
        aliases:
          - "msa_parse"

  msa-snmptrap:
    image: openmsa/openmsa:msa2-snmptrap-2.9.1-ad82a271da1005b784bf69057f10534f73f558ac
    depends_on:
      - db
      - msa-es
      - msa-dev
    healthcheck:
      <<: *healthcheck
      test: ["CMD-SHELL", "/opt/sms/bin/sms -e ISALIVE -t 1 | grep -q OK || exit 1"]
    deploy:
      <<: *placement_rsyslog
    environment:
      <<: *es-configuration
    ports:
      - target: 162
        published: 162
        protocol: udp
        mode: host
    logging:
      <<: *logging
    volumes:
      - "msa_sms_logs:/opt/sms/logs"
      - "/mnt/NASVolume/msa_dev:/opt/devops/"
      - "msa_snmptrapbulkfiles:/opt/sms/spool/parser"
      - "msa_snmptrapbulkfiles_err:/opt/sms/spool/parser-error"
    networks:
      default:
        aliases:
          - "msa_snmptrap"

  msa-bud:
    image: openmsa/openmsa:msa2-bud-2.9.1-3fe82ad9d9afe09f0aa33540fe914e6b23a7d8b8
    depends_on:
      - db
    healthcheck:
      <<: *healthcheck
      test: ["CMD-SHELL", "/opt/sms/bin/sms -e ISALIVE -t 1 | grep -q OK || exit 1"]
    environment:
      - CONTAINER_DOCKNAME={{.Task.Name}}.{{.Node.Hostname}}
    deploy:
      <<: *placement_app
    logging:
      <<: *logging
    volumes:
      - "msa_sms_logs:/opt/sms/logs"
    networks:
      default:
        aliases:
          - "msa_bud"

  msa-alarm:
    image: openmsa/openmsa:msa2-alarm-2.9.1-dde9e346a821f27961fbc41fa5ce685f8a1cb50e
    depends_on:
      - db
      - msa-es
      - msa-api
      - msa-dev
    healthcheck:
      <<: *healthcheck
      test: ["CMD-SHELL", "/opt/sms/bin/sms -e ISALIVE -t 1 | grep -q OK || exit 1"]
    deploy:
      <<: *placement_app
    environment:
      <<: *es-configuration
      CONTAINER_DOCKNAME: "{{.Task.Name}}.{{.Node.Hostname}}"
    volumes:
      - "msa_sms_logs:/opt/sms/logs"
      - "msa_alarmbulkfiles:/opt/sms/spool/alarms"
      - "msa_alarmbulkfiles_err:/opt/sms/spool/alarms-error"
    logging:
      <<: *logging
    networks:
      default:
        aliases:
          - "msa_alarm"

  msa-monitoring:
    image: openmsa/openmsa:msa2-monitoring-2.9.1-917e4e6156d78335644c6612d15e92dfc57f9752
    depends_on:
      - db
      - msa-es
      - msa-dev
      - msa-sms
    healthcheck:
      <<: *healthcheck
      test: ["CMD-SHELL", "/opt/sms/bin/sms -e ISALIVE -t 1 | grep -q OK || exit 1"]
    deploy:
      <<: *placement_app_one_replica
    environment:
      <<: *es-configuration
      CONTAINER_DOCKNAME: "{{.Task.Name}}.{{.Node.Hostname}}"
    volumes:
      - "msa_sms_logs:/opt/sms/logs"
      - "/mnt/NASVolume/msa_dev:/opt/devops/"
      - "/mnt/NASVolume/msa_entities:/opt/fmc_entities"
      - "/mnt/NASVolume/msa_repository:/opt/fmc_repository"
      - "/mnt/NASVolume/rrd_repository:/opt/rrd"
      - "msa_monitbulkfiles:/opt/sms/spool/parser"
      - "msa_monitbulkfiles_err:/opt/sms/spool/parser-error"
    logging:
      <<: *logging
    networks:
      default:
        aliases:
          - "msa_monitoring"

  kafka:
    image: bitnami/kafka:3.5
    healthcheck:
      <<: *healthcheck
    deploy:
      <<: *placement_app
    ports:
      - "9094:9094"
    environment:
      - KAFKA_CFG_NODE_ID=0
      - KAFKA_CFG_PROCESS_ROLES=controller,broker
      - KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=0@kafka:9093
      - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092,CONTROLLER://:9093,EXTERNAL://:9094
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://:9092,EXTERNAL://kafka:9094
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=CONTROLLER:PLAINTEXT,EXTERNAL:PLAINTEXT,PLAINTEXT:PLAINTEXT
      - KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER
      - KAFKA_CFG_INTER_BROKER_LISTENER_NAME=PLAINTEXT
      - KAFKA_CFG_LOG_CLEANER_ENABLE=true
      - KAFKA_CFG_LOG_CLEANUP_POLICY=delete
      - KAFKA_CFG_LOG_RETENTION_BYTES=2000000000
      - KAFKA_CFG_LOG_RETENTION_MS=86400000
      - KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE=true
      - KAFKA_CFG_MESSAGE_MAX_BYTES=1048576
    logging:
      <<: *logging
    volumes:
      - "/mnt/NASVolume/kafka_data:/bitnami/kafka"

  msa-broker:
    image: openmsa/openmsa:msa2-broker-2.9.1-dd05a6145436397696dd81f90c78865bf2106a3b
    healthcheck:
      <<: *healthcheck
    deploy:
      <<: *placement_app
    environment:
      ARTEMIS_PASSWORD: simetraehcapa
      ARTEMIS_USER: artemis
      EXTRA_ARGS: --http-host 0.0.0.0 --relax-jolokia --clustered --addresses core-engine.syslog:anycast --queues core-engine.syslog:anycast
    logging:
      <<: *logging
    volumes:
      - "/mnt/NASVolume/mano_artemis:/var/lib/artemis-instance"

  camunda:
    image: openmsa/openmsa:msa2-camunda-2.9.1-c6acd049f20fe32f3575b961453d8dfdb06b3bf3
    depends_on:
      - db
    healthcheck:
      <<: *healthcheck
    deploy:
      <<: *placement_app
    environment:
      DB_DRIVER: org.postgresql.Driver
      DB_URL: 'jdbc:postgresql://db:5432/process-engine'
      DB_USERNAME: camunda
      DB_PASSWORD: camunda
      DB_VALIDATION_QUERY: 'SELECT 1'
      DB_VALIDATE_ON_BORROW: 'true'
      WAIT_FOR: 'db:5432'
      WAIT_FOR_TIMEOUT: 60
    logging:
      <<: *logging

  msa-kibana:
    image: openmsa/openmsa:msa2-kibana-2.9.1-fdc870f5d050b79f67eee37b79226fac78b1adca
    healthcheck:
      <<: *healthcheck
    deploy:
      <<: *placement_app
    environment:
      ELASTICSEARCH_URL: "http://msa-es:9200"
      ELASTICSEARCH_HOSTS: "http://msa-es:9200"
      <<: *es-configuration
    ports:
      - "5601:5601"
    logging:
      <<: *logging
    networks:
      default:
        aliases:
          - "msa_kibana"

  msa-es:
    image: openmsa/openmsa:msa2-es-2.9.1-be1b9eff5e3f427298d4d1d5a5cb9ba8d2dde3cd
    healthcheck:
      <<: *healthcheck
      test: ["CMD-SHELL", "test -f /home/install/init-done && curl -s -XGET -H 'Authorization: Basic c3VwZXJ1c2VyOnheWnl1R002fnU9K2ZZMkc='  'http://localhost:9200/_cluster/health?pretty' | grep -q 'status.*green' || exit 1"]
    deploy:
      <<: *placement_app
    environment:
      discovery.type: "single-node"
      script.painless.regex.enabled: "true"
      bootstrap.memory_lock: "true"
      xpack.security.enabled: "true"
      ES_JAVA_OPTS: "-Xms512m -Xmx1024m"
      <<: *es-configuration
    ulimits:
      memlock:
        soft: -1
        hard: -1
    networks:
      default:
        aliases:
          - "msa_es"
    volumes:
      - "/mnt/NASVolume/msa_es:/usr/share/elasticsearch/data"
    logging:
      <<: *logging

  msa-cerebro:
    image: openmsa/openmsa:msa2-cerebro-2.9.1-a1d02c6e78123039941ec968a98a9db99fb7f1a6
    healthcheck:
      <<: *healthcheck
    deploy:
      <<: *placement_app
    environment:
      AUTH_TYPE: basic
      BASIC_AUTH_USER: cerebro
      BASIC_AUTH_PWD: "N@X{M4tfw'5%)+35"
    entrypoint:
      - /opt/cerebro/bin/cerebro
      - -Dhosts.0.host=http://msa-es:9200
    ports:
      - "9000:9000"
    logging:
      <<: *logging
    networks:
      default:
        aliases:
          - "msa_cerebro"

  msa-dev:
    image: openmsa/openmsa:msa2-linuxdev-2.9.1-541f58c5bd1e07e0722b1c17ad6a8e2ba72f57c8
    healthcheck:
      <<: *healthcheck
    deploy:
      <<: *placement_app
    volumes:
      - "/mnt/NASVolume/msa_entities:/opt/fmc_entities"
      - "/mnt/NASVolume/msa_repository:/opt/fmc_repository"
      - "/mnt/NASVolume/msa_dev:/opt/devops/"
      - "/mnt/NASVolume/msa_svn:/opt/svnroot"
      - "/mnt/NASVolume/msa_api:/opt/ubi-jentreprise/generated/conf"
      - "/mnt/NASVolume/msa_svn_ws:/opt/sms/spool/routerconfigs"
    logging:
      <<: *logging
    networks:
      default:
        aliases:
          - "msa_dev"

  msa2-es-ilm:
    image: openmsa/openmsa:msa2-es-ilm-2.9.1-dda9094825d80174afde4d18a9fafcadedb1f7d1
    tty: true
    init: true
    deploy:
      replicas: 0
      placement:
        max_replicas_per_node: 1
    networks:
      default:
        aliases:
          - "msa2_es-ilm"
    healthcheck:
      test: ["CMD-SHELL", "find /opt/msa2-es-ilm/log/log_retention.log -type f -mmin -10"]
    depends_on:
      - msa-es
    environment:
      ELASTICSEARCH_URL: "msa_es:9200"
      #For elasticsearch scripts /opt/ubi-es-ilm/log_retention_management.php
      #UBI_ES_INDEX_MULTIPLE_TTL:        "type:traffic|7d,type:event|30d,*|90d"
      UBI_ES_INDEX_MULTIPLE_TTL:         "*|90d"
      UBI_ES_AUDIT_INDEX_MULTIPLE_TTL:   "*|90d"
      UBI_ES_LOG_SEARCH_INDEX_LIST:      "ubilogs"
      UBI_ES_RETENTION_INDEX_NAME:       "ubilogs*"
      UBI_ES_RETENTION_AUDIT_INDEX_NAME: "ubiaudit*"
      UBI_ES_RETENTION_ALARM_INDEX_NAME: "ubialarm*"
      UBI_ES_ALARM_INDEX_MULTIPLE_TTL:   "*|90d"
      UBI_ES_CACHE_INDEX_DEFAULT_TTL:    "1w"
      UBI_ES_DELETE_SCROLL_SIZE:         "4000"
      UBI_ES_MAX_DOCS:                   ""
      UBI_ES_LOG_DETENTION_DELETE:       "true"
      UBI_ES_ILM_LOG_CRONTAB:            "*/2 * * * *  php /opt/ubi-es-ilm/log_retention_management.php --verbose=3 > /proc/1/fd/1 2>&1"
      <<: *es-configuration
    volumes:
      - "/mnt/NASVolume/msa2_es-ilm:/opt/msa2-es-ilm"

  msa-smtp:
    image: boky/postfix:4.2.1-alpine
    environment:
      HOSTNAME: msa-smtp
      # RELAYHOST: <MTA ip>:25
      POSTFIX_message_size_limit: 2097152
      ALLOW_EMPTY_SENDER_DOMAINS: "true"
      ALLOWED_SENDER_DOMAINS: ''

volumes:
  msa_sms_logs:
  msa_monitbulkfiles:
  msa_monitbulkfiles_err:
  msa_parsebulkfiles:
  msa_parsebulkfiles_err:
  msa_alarmbulkfiles:
  msa_alarmbulkfiles_err:
  msa_bulkfiles:
  msa_bulkfiles_err:
  msa_snmptrapbulkfiles:
  msa_snmptrapbulkfiles_err:
  msa_es:
  msa_es_config:
  msa_api_logs: 

networks:
  default:
#    driver_opts:
#      encrypted: "true"
