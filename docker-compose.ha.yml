# This docker-compose file is provided as an example to create a Docker Swarm based MSActivator setup
x-amqp-monitoring: &amqp-monitoring
  AMQP_ADDRESS: core-engine.monitoring
  AMQP_PASSWORD: simetraehcapa
  AMQP_PORT: "5672"
  AMQP_SERVER: msa-broker
  AMQP_USER: artemis
x-amqp-syslog: &amqp-syslog
  AMQP_ADDRESS: core-engine.syslog
  AMQP_PASSWORD: simetraehcapa
  AMQP_PORT: "5672"
  AMQP_SERVER: msa-broker
  AMQP_USER: artemis
x-es-configuration: &es-configuration
  ES_CREDENTIALS: c3VwZXJ1c2VyOnheWnl1R002fnU9K2ZZMkc=
  ES_SERVERS: msa-es
x-healthcheck: &healthcheck
  interval: 30s
  retries: 10
  start_period: 120s
  timeout: 10s
x-logging: &logging
  driver: json-file
  options:
    max-buffer-size: 4m
    max-file: "5"
    max-size: 10m
    mode: non-blocking
x-placement_app: &placement_app
  placement:
    constraints:
      - node.labels.worker==app
    max_replicas_per_node: 1
  replicas: 1
# for containers that have to run with only one replica
x-placement_app_one_replica: &placement_app_one_replica
  placement:
    constraints:
      - node.labels.worker==app
    max_replicas_per_node: 1
  replicas: 1
x-placement_db: &placement_db
  placement:
    constraints:
      - node.labels.worker==db
    max_replicas_per_node: 1
  replicas: 1
# snmptrap and rsyslog ports are in mode host so we have to configure one replica per worker
x-placement_rsyslog: &placement_rsyslog
  placement:
    constraints:
      - node.labels.worker==app
    max_replicas_per_node: 1
  replicas: 1
networks:
  default:
#    driver_opts:
#      encrypted: "true"

services:
  msa-alarm:
    depends_on:
      - msa-db
      - msa-es
      - msa-api
    deploy:
      <<: *placement_app
    environment:
      <<: *es-configuration
      CONTAINER_DOCKNAME: '{{.Task.Name}}.{{.Node.Hostname}}'
    healthcheck:
      <<: *healthcheck
      test:
        - CMD-SHELL
        - /opt/sms/bin/sms -e ISALIVE -t 1 | grep -q OK || exit 1
    image: openmsa/openmsa:msa2-alarm-2.9.1-sha-029276dc72def6b1ea65904e602641da69e2f546
    logging:
      <<: *logging
    networks:
      default:
        aliases:
          - msa_alarm
    volumes:
      - msa_sms_logs:/opt/sms/logs
      - msa_alarmbulkfiles:/opt/sms/spool/alarms
      - msa_alarmbulkfiles_err:/opt/sms/spool/alarms-error
  msa-api:
    depends_on:
      - msa-db
    deploy:
      <<: *placement_app
    environment:
      <<: *es-configuration
      CONTAINER_DOCKNAME: '{{.Task.Name}}.{{.Node.Hostname}}'
      HOST_HOSTNAME: '{{.Node.Hostname}}'
    healthcheck:
      <<: *healthcheck
      test:
        - CMD-SHELL
        - curl -s --fail http://localhost:8480/actuator/health |jq -r '.status' |grep '^UP$$'
    image: openmsa/openmsa:msa2-api-2.9.1-sha-2378fa24d11123868fc5a5eeea33fa7ff0aa2123
    logging:
      <<: *logging
    networks:
      default:
        aliases:
          - msa_api
    volumes:
      - /mnt/NASVolume/msa_dev:/opt/devops/
      - /mnt/NASVolume/rrd_repository:/opt/rrd
      - /mnt/NASVolume/msa_entities:/opt/fmc_entities
      - /mnt/NASVolume/msa_repository:/opt/fmc_repository
      - /mnt/NASVolume/msa_api_keystore:/etc/pki/jentreprise
      - /mnt/NASVolume/msa_api_logs:/opt/wildfly/logs
  msa-auth:
    depends_on:
      - msa-db
    environment:
      KEYCLOAK_ADMIN: admin
      KEYCLOAK_ADMIN_PASSWORD: ubiqube
    healthcheck:
      interval: 30s
      retries: 5
      start_period: 1m
      test: timeout 10s bash -c ':> /dev/tcp/127.0.0.1/8080'
      timeout: 10s
    image: openmsa/openmsa:msa2-auth-2.9.1-sha-682c8fa153a2452c2ad56114d4281b692456921b
  msa-broker:
    deploy:
      <<: *placement_app
    environment:
      ARTEMIS_PASSWORD: simetraehcapa
      ARTEMIS_USER: artemis
      EXTRA_ARGS: --http-host 0.0.0.0 --relax-jolokia --clustered --addresses core-engine.syslog:anycast --queues core-engine.syslog:anycast
    healthcheck:
      <<: *healthcheck
      test: ./bin/artemis check node --user=$${ARTEMIS_USER} --password=$${ARTEMIS_PASSWORD} --silent &>/dev/null
    image: openmsa/openmsa:msa2-broker-2.9.1-sha-c7b473f778eb2cb1d5f5a6496e6064ed0bdec0a8
    logging:
      <<: *logging
    volumes:
      - /mnt/NASVolume/mano_artemis:/var/lib/artemis-instance
  msa-bud:
    depends_on:
      - msa-db
    deploy:
      <<: *placement_app
    environment:
      - CONTAINER_DOCKNAME={{.Task.Name}}.{{.Node.Hostname}}
    healthcheck:
      <<: *healthcheck
      test:
        - CMD-SHELL
        - /opt/sms/bin/sms -e ISALIVE -t 1 | grep -q OK || exit 1
    image: openmsa/openmsa:msa2-bud-2.9.1-sha-8e5656b8c8968e6f6bc8d499a66082cbfb1fea59
    logging:
      <<: *logging
    networks:
      default:
        aliases:
          - msa_bud
    volumes:
      - msa_sms_logs:/opt/sms/logs
  msa-camunda:
    depends_on:
      - msa-db
    deploy:
      <<: *placement_app
    environment:
      DB_DRIVER: org.postgresql.Driver
      DB_PASSWORD: camunda
      DB_URL: jdbc:postgresql://msa-db:5432/process-engine
      DB_USERNAME: camunda
      DB_VALIDATE_ON_BORROW: "true"
      DB_VALIDATION_QUERY: SELECT 1
      WAIT_FOR: msa-db:5432
      WAIT_FOR_TIMEOUT: 60
    healthcheck:
      <<: *healthcheck
      test:
        - CMD-SHELL
        - curl -s --fail http://localhost:8080/actuator/health |jq -r '.status' |grep '^UP$$'
    image: openmsa/openmsa:msa2-camunda-2.9.1-sha-87395dda76e592016374c93b0cb65940f3e5d803
    logging:
      <<: *logging
    networks:
      default:
        aliases:
          - camunda
  msa-cerebro:
    deploy:
      <<: *placement_app
    entrypoint:
      - /opt/cerebro/bin/cerebro
      - -Dhosts.0.host=http://msa-es:9200
    environment:
      AUTH_TYPE: basic
      BASIC_AUTH_PWD: N@X{M4tfw'5%)+35
      BASIC_AUTH_USER: cerebro
    healthcheck:
      <<: *healthcheck
      test: curl --fail http://localhost:9000/
    image: openmsa/openmsa:msa2-cerebro-2.9.1-sha-3438b6a723d22d15afbd877dead9820c6adac735
    logging:
      <<: *logging
    networks:
      default:
        aliases:
          - msa_cerebro
    ports:
      - 9000:9000
  msa-db:
    deploy:
      <<: *placement_db
    environment:
      CAMUNDA_DB: process-engine
      CAMUNDA_PASSWORD: camunda
      CAMUNDA_USER: camunda
      KEY_VAULT_DB: key_vault
      KEY_VAULT_USER: key_vault
      MAX_CONNECTIONS: 1600
      PG_DATABASE: POSTGRESQL
      PG_MODE: primary
      PG_PASSWORD: my_db_password
      PG_PRIMARY_PASSWORD: my_db_password
      PG_PRIMARY_PORT: 5432
      PG_PRIMARY_USER: postgres
      PG_ROOT_PASSWORD: my_db_password
      PG_USER: postgres
    healthcheck:
      <<: *healthcheck
      test:
        - CMD-SHELL
        - /usr/pgsql-12/bin/pg_isready -h localhost
    image: openmsa/openmsa:msa2-db-2.9.1-sha-464da33fdab006c857b2c953f75ec6a12c9cb3fc
    logging:
      <<: *logging
    networks:
      default:
        aliases:
          - db
    volumes:
      - /mnt/NASVolume/msa_db:/pgsqldata/pgsql
      - target: /dev/shm
        tmpfs:
          size: 2000000000
        type: tmpfs
  msa-db-replica:
    deploy:
      <<: *placement_db
    environment:
      CAMUNDA_DB: process-engine
      CAMUNDA_PASSWORD: camunda
      CAMUNDA_USER: camunda
      KEY_VAULT_DB: key_vault
      KEY_VAULT_USER: key_vault
      PG_DATABASE: POSTGRESQL
      PG_MODE: replica
      PG_PASSWORD: my_db_password
      PG_PRIMARY_HOST: msa-db
      PG_PRIMARY_PASSWORD: my_db_password
      PG_PRIMARY_PORT: 5432
      PG_PRIMARY_USER: postgres
      PG_ROOT_PASSWORD: my_db_password
      PG_USER: postgres
    healthcheck:
      <<: *healthcheck
      test:
        - CMD-SHELL
        - /usr/pgsql-12/bin/pg_isready -h localhost
    image: openmsa/openmsa:msa2-db-2.9.1-sha-464da33fdab006c857b2c953f75ec6a12c9cb3fc
    logging:
      <<: *logging
    networks:
      default:
        aliases:
          - db-replica
  msa-dev:
    deploy:
      <<: *placement_app
    healthcheck:
      <<: *healthcheck
    image: openmsa/openmsa:msa2-linuxdev-2.9.1-sha-793fddbbba933bb8b9cd5327581d75e201d976cb
    logging:
      <<: *logging
    networks:
      default:
        aliases:
          - msa_dev
    volumes:
      - /mnt/NASVolume/msa_entities:/opt/fmc_entities
      - /mnt/NASVolume/msa_repository:/opt/fmc_repository
      - /mnt/NASVolume/msa_dev:/opt/devops/
      - /mnt/NASVolume/msa_svn:/opt/svnroot
      - /mnt/NASVolume/msa_api:/opt/ubi-jentreprise/generated/conf
      - /mnt/NASVolume/msa_svn_ws:/opt/sms/spool/routerconfigs
  msa-es:
    deploy:
      <<: *placement_app
    environment:
      <<: *es-configuration
      ES_JAVA_OPTS: -Xms512m -Xmx1024m
      bootstrap.memory_lock: "true"
      discovery.type: single-node
      script.painless.regex.enabled: "true"
      xpack.security.enabled: "true"
    healthcheck:
      <<: *healthcheck
      test:
        - CMD-SHELL
        - 'test -f /home/install/init-done && curl -s -XGET -H ''Authorization: Basic c3VwZXJ1c2VyOnheWnl1R002fnU9K2ZZMkc=''  ''http://localhost:9200/_cluster/health?pretty'' | grep -q ''status.*green'' || exit 1'
    image: openmsa/openmsa:msa2-es-2.9.1-sha-a8b546208942b3c3057e60494575538678a4968d
    logging:
      <<: *logging
    networks:
      default:
        aliases:
          - msa_es
    ulimits:
      memlock:
        hard: -1
        soft: -1
    volumes:
      - /mnt/NASVolume/msa_es:/usr/share/elasticsearch/data
  msa-front:
    depends_on:
      - msa-api
      - msa-ui
      - msa-camunda
    deploy:
      <<: *placement_app
    healthcheck:
      <<: *healthcheck
      test:
        - CMD-SHELL
        - curl -k --fail https://localhost
    image: openmsa/openmsa:msa2-front-2.9.1-sha-0d8b8a61ebb5c83d1b9b87fe4e380c0bda2062fc
    logging:
      <<: *logging
    ports:
      - mode: ingress
        protocol: tcp
        published: 80
        target: 80
      - mode: ingress
        protocol: tcp
        published: 443
        target: 443
    volumes:
      - /mnt/NASVolume/msa_front_conf:/etc/nginx/custom_conf.d
      #
      # uncomment one of the 2 sections below when installing a custom certificate
      # - Docker standard standalone installation
      # volumes:
      #    - "msa_front:/etc/nginx/ssl"
      # - Docker Swarm HA installation
      # volumes:
      #    - "/mnt/NASVolume/msa_front:/etc/nginx/ssl"
  msa-kibana:
    deploy:
      <<: *placement_app
    environment:
      <<: *es-configuration
      ELASTICSEARCH_HOSTS: http://msa-es:9200
      ELASTICSEARCH_URL: http://msa-es:9200
    healthcheck:
      <<: *healthcheck
      test: curl -k --fail http://localhost:5601/kibana/
    image: openmsa/openmsa:msa2-kibana-2.9.1-sha-fbe20e11564c106b789983b24900186acfe81036
    logging:
      <<: *logging
    networks:
      default:
        aliases:
          - msa_kibana
    ports:
      - 5601:5601
  msa-mongodb:
    env_file:
      - .env
    #      ccla must match user created in mongo-init.js
    environment:
      MONGO_INITDB_ROOT_PASSWORD: ${MONGO_INITDB_ROOT_PASSWORD:-my_db_password}
      MONGO_INITDB_ROOT_USERNAME: ${MONGO_INITDB_ROOT_USERNAME:-admin}
    healthcheck:
      test: echo 'db.runCommand("ping").ok' | mongosh mongodb://${DB_USER:-msaUser}:${DB_PASSWORD:-ubiqube38}@localhost:${DB_PORT:-27017}/${DB_NAME:-msa} --quiet
    image: openmsa/openmsa:msa2-mongodb-2.9.1-sha-90b2c29c1e13bc7b82cfadb00cba97ac856fd642
    ports:
      - 27017:27017
    restart: unless-stopped
    volumes:
      - /mnt/NASVolume/msa_mongodb_data:/data/db
      - ./mongo/mongo-init.js:/docker-entrypoint-initdb.d/mongo-init.js:ro
  msa-monitoring:
    depends_on:
      - msa-db
      - msa-es
      - msa-dev
      - msa-sms
    deploy:
      <<: *placement_app_one_replica
    environment:
      <<:
        - *es-configuration
        - *amqp-monitoring
      CONTAINER_DOCKNAME: '{{.Task.Name}}.{{.Node.Hostname}}'
      TARGET_MON: RRD
      #   possible values RRD, ES, AMQP
    healthcheck:
      <<: *healthcheck
      test:
        - CMD-SHELL
        - /opt/sms/bin/sms -e ISALIVE -t 1 | grep -q OK || exit 1
    image: openmsa/openmsa:msa2-monitoring-2.9.1-sha-b5054bf89e55f3e065db4a0b66784da3a827924d
    logging:
      <<: *logging
    networks:
      default:
        aliases:
          - msa_monitoring
    volumes:
      - msa_sms_logs:/opt/sms/logs
      - /mnt/NASVolume/msa_dev:/opt/devops/
      - /mnt/NASVolume/msa_entities:/opt/fmc_entities
      - /mnt/NASVolume/msa_repository:/opt/fmc_repository
      - /mnt/NASVolume/rrd_repository:/opt/rrd
      - msa_monitbulkfiles:/opt/sms/spool/parser
      - msa_monitbulkfiles_err:/opt/sms/spool/parser-error
  msa-parse:
    depends_on:
      - msa-db
      - msa-broker
      - msa-es
      - msa-dev
    deploy:
      <<: *placement_app
    environment:
      <<:
        - *es-configuration
        - *amqp-syslog
    healthcheck:
      <<: *healthcheck
      test:
        - CMD-SHELL
        - /opt/sms/bin/sms -e ISALIVE -t 1 | grep -q OK || exit 1
    image: openmsa/openmsa:msa2-parse-2.9.1-sha-9beb01d1f84dd085e347cb9714dbcaddad791974
    logging:
      <<: *logging
    networks:
      default:
        aliases:
          - msa_parse
    volumes:
      - msa_sms_logs:/opt/sms/logs
      - /mnt/NASVolume/msa_dev:/opt/devops/
      - msa_parsebulkfiles:/opt/sms/spool/parser
      - msa_parsebulkfiles_err:/opt/sms/spool/parser-error
  msa-rsyslog:
    depends_on:
      - msa-broker
    deploy:
      <<: *placement_rsyslog
    environment:
      <<: *amqp-syslog
      ACTIONTYPE: omamqp1
    healthcheck:
      <<: *healthcheck
      test:
        - CMD-SHELL
        - ps -p 1 -h -o%cpu | awk '{if ($$1 > 99) exit 1; else exit 0;}'
    image: openmsa/openmsa:msa2-rsyslog-2.9.1-sha-a197212ba58e60726fc5ca2f493c1045acd0c47e
    logging:
      <<: *logging
    networks:
      default:
        aliases:
          - msa_rsyslog
    ports:
      # on docker swarm rsyslog port can support only one protocol (TCP or UDP) per port and MUST be in host mode
      - mode: host
        protocol: udp
        published: 514
        target: 514
      - mode: host
        protocol: tcp
        published: 6514
        target: 6514
  msa-sms:
    depends_on:
      - msa-db
      - msa-dev
    deploy:
      <<: *placement_app
    environment:
      <<: *es-configuration
      CONTAINER_DOCKNAME: '{{.Task.Name}}.{{.Node.Hostname}}'
      HOST_HOSTNAME: '{{.Node.Hostname}}'
    healthcheck:
      <<: *healthcheck
      test:
        - CMD-SHELL
        - /opt/sms/bin/sms -e ISALIVE -t 1 | grep -q OK || exit 1
    image: openmsa/openmsa:msa2-sms-2.9.1-sha-eaf48e66d7a86cfc4a26079033bccce09a90e362
    logging:
      <<: *logging
    networks:
      default:
        aliases:
          - msa_sms
    ports:
      - mode: host
        protocol: udp
        published: 69
        target: 69
      - mode: host
        protocol: udp
        published: 5200
        target: 5200
    volumes:
      - /mnt/NASVolume/msa_sms_logs:/opt/sms/logs
      - /mnt/NASVolume/msa_dev:/opt/devops/
      - /mnt/NASVolume/msa_entities:/opt/fmc_entities
      - /mnt/NASVolume/msa_repository:/opt/fmc_repository
      - /mnt/NASVolume/msa_svn:/opt/svnroot
      - msa_bulkfiles:/opt/sms/spool/parser
      - msa_bulkfiles_err:/opt/sms/spool/parser-error
  msa-smtp:
    environment:
      ALLOWED_SENDER_DOMAINS: ""
      ALLOW_EMPTY_SENDER_DOMAINS: "true"
      HOSTNAME: msa-smtp
      # RELAYHOST: <MTA ip>:25
      POSTFIX_message_size_limit: 2097152
    image: boky/postfix:4.2.1-alpine
  msa-snmptrap:
    depends_on:
      - msa-db
      - msa-es
      - msa-dev
    deploy:
      <<: *placement_rsyslog
    environment:
      <<: *es-configuration
    healthcheck:
      <<: *healthcheck
      test:
        - CMD-SHELL
        - /opt/sms/bin/sms -e ISALIVE -t 1 | grep -q OK || exit 1
    image: openmsa/openmsa:msa2-snmptrap-2.9.1-sha-942e1c4b7a65648ee8d28b4424fd3e3fdee6f8d5
    logging:
      <<: *logging
    networks:
      default:
        aliases:
          - msa_snmptrap
    ports:
      - mode: host
        protocol: udp
        published: 162
        target: 162
    volumes:
      - msa_sms_logs:/opt/sms/logs
      - /mnt/NASVolume/msa_dev:/opt/devops/
      - msa_snmptrapbulkfiles:/opt/sms/spool/parser
      - msa_snmptrapbulkfiles_err:/opt/sms/spool/parser-error
  msa-ui:
    depends_on:
      - msa-api
    deploy:
      <<: *placement_app
    environment:
      - FEATURE_ADMIN=true
      - FEATURE_REPOSITORY=true
      - FEATURE_CONNECTION_STATUS=true
      - FEATURE_ALARMS=true
      - FEATURE_LICENCE=true
      - FEATURE_TOPOLOGY=true
      - FEATURE_MONITORING_PROFILES=true
      - FEATURE_PROFILE_AUDIT_LOGS=true
      - FEATURE_PERMISSION_PROFILES=true
      - FEATURE_AI_ML=false
      - FEATURE_MICROSERVICE_BULK_OPERATION=false
      - FEATURE_EDIT_VARIABLES_IN_MICROSERVICE_CONSOLE=true
      - FEATURE_WORKFLOW_OWNER=false
      - FEATURE_PERMISSION_PROFILE_LABELS=false
      - FEATURE_BPM=true
      - UBIQUBE_ES_SECURITY_DISABLED=true
      - FEATURE_ALARMS_AUTO_CLEARANCE=false
      - FEATURE_IMPORT_WITH_SAME_AND_UPPERRANK=true
      - FEATURE_REPOFOLDERLIST=[\"Datafiles\"]
    healthcheck:
      <<: *healthcheck
      test:
        - CMD-SHELL
        - curl --fail http://localhost:8080
    image: openmsa/openmsa:msa2-ui-2.9.1-73236bfbb2c39debed56ed1a55045b48f9373bde
    logging:
      <<: *logging
    networks:
      default:
        aliases:
          - msa_ui
  msa2-es-ilm:
    depends_on:
      - msa-es
    deploy:
      placement:
        max_replicas_per_node: 1
      replicas: 0
    environment:
      <<: *es-configuration
      ELASTICSEARCH_URL: msa_es:9200
      UBI_ES_ALARM_INDEX_MULTIPLE_TTL: '*|90d'
      UBI_ES_AUDIT_INDEX_MULTIPLE_TTL: '*|90d'
      UBI_ES_CACHE_INDEX_DEFAULT_TTL: 1w
      UBI_ES_DELETE_SCROLL_SIZE: "4000"
      UBI_ES_ILM_LOG_CRONTAB: '*/2 * * * * root php /opt/ubi-es-ilm/log_retention_management.php --verbose=3 > /proc/1/fd/1 2>&1'
      # For elasticsearch scripts /opt/ubi-es-ilm/log_retention_management.php
      # UBI_ES_INDEX_MULTIPLE_TTL:        "type:traffic|7d,type:event|30d,*|90d"
      UBI_ES_INDEX_MULTIPLE_TTL: '*|90d'
      UBI_ES_LOG_DETENTION_DELETE: "true"
      UBI_ES_LOG_SEARCH_INDEX_LIST: ubilogs
      UBI_ES_MAX_DOCS: ""
      UBI_ES_RETENTION_ALARM_INDEX_NAME: ubialarm*
      UBI_ES_RETENTION_AUDIT_INDEX_NAME: ubiaudit*
      UBI_ES_RETENTION_INDEX_NAME: ubilogs*
    healthcheck:
      test:
        - CMD-SHELL
        - find /opt/msa2-es-ilm/log/log_retention.log -type f -mmin -10
    image: openmsa/openmsa:msa2-es-ilm-2.9.1-sha-6071ae10ede7a06b1982210b7da962ac862586bb
    networks:
      default:
        aliases:
          - msa2_es-ilm
    volumes:
      - /mnt/NASVolume/msa2_es-ilm:/opt/msa2-es-ilm
volumes:
  msa_alarmbulkfiles:
  msa_alarmbulkfiles_err:
  msa_api_logs:
  msa_bulkfiles:
  msa_bulkfiles_err:
  msa_monitbulkfiles:
  msa_monitbulkfiles_err:
  msa_parsebulkfiles:
  msa_parsebulkfiles_err:
  msa_sms_logs:
  msa_snmptrapbulkfiles:
  msa_snmptrapbulkfiles_err:
